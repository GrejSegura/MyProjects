{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation Process\n",
    "==================\n",
    "**Prepared by** : Grej - Mar 11, 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import geopy.distance\n",
    "import math\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will check the number of registrations per company, this will be used as a new feature.\n",
    "\n",
    "# count the number of registrants per company - categories are combined company name + country, country + website and website only\n",
    "\n",
    "def company_reg_counts(data, data_type):\n",
    "    os.chdir(r'C:\\Users\\User\\Documents\\Data_Science_Projects\\attendee-prediction-project')\n",
    "    data_path = data_type+'_raw_data.csv'\n",
    "    # encoding = latin-1 was used here due to the characters that are unreadable when using the standard utf-8\n",
    "    data = pd.read_csv(r'.\\\\data\\\\'+data_path, encoding='latin-1')\n",
    "    data = data[['id', 'Company Name', 'Country', 'Website']]\n",
    "    data['country_company'] = data[['Company Name', 'Country']].apply(lambda x: r''.join(x.astype(str)), axis=1).str.lower().str.replace(r' ', r'')\n",
    "    count_per_company = data[data['Company Name']!=' '].country_company.value_counts().rename_axis('x').reset_index(name='count_per_company')\n",
    "    data['country_website'] = data[['Website', 'Country']].apply(lambda x: r''.join(x.astype(str)), axis=1).str.lower().str.replace(r' ', r'')\n",
    "    count_per_comp_website = data[data['Website']!=' '].country_company.value_counts().rename_axis('y').reset_index(name='count_per_comp_website')\n",
    "    count_per_website = data[data['Website']!=' '].Website.value_counts().rename_axis('z').reset_index(name='count_per_website')\n",
    "    \n",
    "    data = data.merge(count_per_company, left_on = 'country_company', right_on = 'x', how = 'left')\n",
    "    data = data.merge(count_per_comp_website, left_on = 'country_website', right_on = 'y', how = 'left')\n",
    "    data = data.merge(count_per_website, left_on = 'Website', right_on = 'z', how = 'left')\n",
    "    company_reg_counts_data = data[['id', 'count_per_company', 'count_per_comp_website', 'count_per_website']].fillna(1)\n",
    "    company_reg_counts_data.to_pickle(r'.\\data\\output\\company_reg_count.pkl')\n",
    "    return company_reg_counts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_type):\n",
    "    os.chdir(r'C:\\Users\\User\\Documents\\Data_Science_Projects\\attendee-prediction-project')\n",
    "    data_path = data_type+'_raw_data.csv'\n",
    "    # encoding = latin-1 was used here due to the characters that are unreadable when using the standard utf-8\n",
    "    data = pd.read_csv(r'.\\\\data\\\\'+data_path, encoding='latin-1')\n",
    "\n",
    "    # add show columns\n",
    "    data['show'] = 'event_1'\n",
    "    data['show_date'] = pd.to_datetime('4/22/2018')\n",
    "\n",
    "    data = data.rename(columns={'Responses':'merged'})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dummify all responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummify_responses(data, codes):\n",
    "    columns_to_check = {'merged', 'id'}\n",
    "    for cols in columns_to_check:\n",
    "        if cols not in data.columns.values:\n",
    "            print('There is no ',cols,' column')\n",
    "            break\n",
    "    else:\n",
    "        responses = data['merged'].str.split(r']', expand=True)\n",
    "        print('checkpoint 1 -- '+ str(len(responses)))\n",
    "        responses['10 Digit Card Number'] = data['id']\n",
    "        print('checkpoint 2 -- '+ str(len(responses)))\n",
    "        responses = responses.melt(id_vars=['id'], value_name = 'code')\n",
    "        responses['value'] = 1\n",
    "        print('checkpoint 3 -- '+ str(len(responses)))\n",
    "        \n",
    "        responses = responses.merge(codes, left_on = 'code', right_on = 'code', how = 'left')\n",
    "        print('checkpoint 4 -- '+ str(len(responses)))\n",
    "                \n",
    "        responses = responses.loc[responses['included'] == 'YES']\n",
    "        print('checkpoint 5 -- '+ str(len(responses)))\n",
    "        responses = responses.drop(['show', 'question', 'code', 'text_answer', 'included', 'job_rank'], axis=1)\n",
    "        print('checkpoint 6 -- '+ str(len(responses)))\n",
    "        \n",
    "        responses = responses.pivot_table(index = 'id', columns = 'decode', values = 'value', aggfunc = 'max')\n",
    "        print('checkpoint 7 -- '+ str(len(responses)))\n",
    "\n",
    "        #responses.loc[responses['Attended']!=1, 'Attended'] = 0\n",
    "        return responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add email and website features\n",
    "This part is a feature engineering process.  \n",
    "The logic behind is that registrants that has entered websites and emails might have correlation to those who attend.  \n",
    "One reason might be because those who have websites and emails are more interested and their company's are active in the industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance of the country B from country A might be also a factor.  \n",
    "The hypothesis is those who come from farther places are less likely to attend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate distance of country from USA.\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    from math import radians, cos, sin, asin, sqrt\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_website(data):\n",
    "    if 'Website' in data.columns.values:\n",
    "        data.loc[data['Website']==\" \", 'with_website'] = 0\n",
    "        data.loc[data['Website']!=\" \", 'with_website'] = 1\n",
    "        return data\n",
    "    else:\n",
    "        print('There is no website column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add days_to_go and weeks_to_go feature\n",
    "The hypothesis is those who register close to the date of the show are more likely to attend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_to_go_reg(data):\n",
    "    if 'Date Created' in data.columns.values:\n",
    "        data['Date Created'] = pd.to_datetime(data['Date Created'])\n",
    "        difference = data['show_date'] - data['Date Created']\n",
    "        return difference\n",
    "    else:\n",
    "        print('There is no Date Created column')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create groupings for UAE States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_group(data):\n",
    "    if 'State' in data.columns.values:\n",
    "        data['State'] = data['State'].str.lower().str.replace(r' ', r'_')\n",
    "        data.loc[data['country']!= 'USA', 'State'] = 'international_state'\n",
    "        data.loc[data['State']== r' ', 'State'] = 'dubai'\n",
    "        data.loc[data['State']== r'_', 'State'] = 'dubai'\n",
    "        print('\\n State Summary\\n')\n",
    "        print(data['State'].value_counts())\n",
    "        return data\n",
    "    else:\n",
    "        print('There is no State column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleanup the country and group into regions, add the distance of countries from UAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_country(data, region):\n",
    "    if 'Country' in data.columns.values:\n",
    "        data.loc[pd.isnull(data['Country']), 'Country'] = 'United Arab Emirates' # replace blank countries with UAE\n",
    "        data.loc[data['Country']=='', 'Country'] = 'USA' # replace blank countries with UAE\n",
    "        data.loc[data['Country']==' ', 'Country'] = 'USA' # replace blank countries with UAE\n",
    "        \n",
    "        data = data.merge(region, left_on = 'Country', right_on = 'country', how = 'left')\n",
    "        no_region = data.loc[pd.isnull(data['region_2'])]\n",
    "        print('\\n Region Summary\\n')\n",
    "        print(data['region_2'].value_counts())\n",
    "        print('\\nNumber of No Region -- '+ str(len(no_region)))\n",
    "        return data, no_region\n",
    "    else:\n",
    "        print('There is no Country column')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_encode(data, columns):\n",
    "    for cols in columns:\n",
    "        data['mean_encode_'+cols] = data[cols].map(data.groupby(cols)['attended'].mean())\n",
    "        print('\\nMean Encoder Summary -- '+cols)\n",
    "        print(data['mean_encode_'+cols].describe())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dummify all other categorical variables not included in responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummify_columns(data, columns):\n",
    "    columns_to_check = {'id'}\n",
    "    for cols in columns_to_check:\n",
    "        if cols not in data.columns.values:\n",
    "            print('There is no ',cols,' column')\n",
    "            break\n",
    "    data_1 = data[columns]\n",
    "    data_1 = pd.get_dummies(data_1[columns])\n",
    "    data_1['id'] = data['id']\n",
    "    data = data.merge(data_1, on='id', how = 'left')\n",
    "    data = data.drop(columns, axis=1)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # type of data to be processed\n",
    "    data_type = 'train'\n",
    "    \n",
    "    # load data\n",
    "    data = load_data(data_type)\n",
    "    \n",
    "    company_reg_counts_data = company_reg_counts(data, data_type)\n",
    "    \n",
    "    data = data.merge(company_reg_counts_data,  on='id', how = 'left')\n",
    "    data['company_name'] = data[['Company Name']].apply(lambda x: r''.join(x.astype(str)), axis=1).str.lower().str.replace(r' ', r'')\n",
    "    \n",
    "    print('data length is '+ str(len(data)))\n",
    "    # Load the codes data and one-hot-code all responses\n",
    "    codes = pd.read_excel(r'.\\data\\codes.xlsx')\n",
    "    \n",
    "    responses = dummify_responses(data, codes)\n",
    "    data = data.merge(responses, on='id', how = 'left').drop('merged', axis=1)\n",
    "\n",
    "    data = data.fillna(0)\n",
    "\n",
    "    # add with_website, with_email, days_to_go, weeks_to_go feature\n",
    "    data = with_website(data)\n",
    "    ##data = with_email(data)\n",
    "    data = data.drop(['Email', 'Website'], axis=1)\n",
    "    difference = days_to_go_reg(data)\n",
    "    data['days_to_go'] = difference.dt.days\n",
    "    data['weeks_to_go'] = round(data['days_to_go']/7)\n",
    "    data = data.drop(['Date Created', 'show_date'], axis=1)\n",
    "\n",
    " \n",
    "    # Load regions data and cleanup the country\n",
    "    region = pd.read_excel(r'.\\data\\region.xlsx')\n",
    "    data, no_region = cleanup_country(data, region)\n",
    "        \n",
    "    # create the State binning feature\n",
    "    data = create_state_group(data)\n",
    "\n",
    "    # create mean encoded features for ['State', 'region_1', 'region_2']\n",
    "    \n",
    "    data = mean_encode(data, columns = ['State', 'country', 'region_1', 'region_2', 'days_to_go'])\n",
    "  \n",
    "    columns = ['State', 'region_1', 'region_2']\n",
    "    data = dummify_columns(data, columns)\n",
    "    data = data.fillna(0)\n",
    "    \n",
    "    # add distance feature using latitude and longitude\n",
    "    distance = pd.Series([])\n",
    "    for i in range(len(data.index)):\n",
    "        lon1 = 53.847818\n",
    "        lat1 = 23.424076\n",
    "        lon2 = data.loc[i,['longitude']]\n",
    "        lat2 = data.loc[i,['latitude']]\n",
    "        dist = pd.Series(haversine(lon1, lat1, lon2, lat2))\n",
    "        distance = distance.append(dist, ignore_index=True)\n",
    "    data['distance'] = distance\n",
    "    \n",
    "    # below are situation-based wrangling of data - \n",
    "    # removed and filtered some features that are not used anymore\n",
    "    \n",
    "    data = data.drop(['country', 'latitude', 'longitude', 'with_website', 'show', 'Country', 'company_name'], axis=True)\n",
    "    data = data.drop('Company Name', axis=True)\n",
    "    data = data[data['days_to_go']>=1].reset_index().drop('index', axis=1)\n",
    "    \n",
    "    data = data.rename(columns={'attended': 'target', 'id':'card_number'})\n",
    "    \n",
    "\n",
    "    # saving the data as cleanData\n",
    "    data.to_csv(r'.\\data\\output\\train_clean_data.csv', index=False)\n",
    "    \n",
    "    # identify the features - important to have the same feature for the test Data\n",
    "    columns = pd.DataFrame({'cols' : data.columns})\n",
    "    columns = columns[1:]\n",
    "    columns = columns[columns['cols']!='target']\n",
    "    columns.to_csv(r'.\\data\\output\\columns_used_for_model.csv', index=False)\n",
    "    print('\\nNumber of Features -- '+ str(len(columns)))\n",
    "    \n",
    "    print('\\nDone...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
