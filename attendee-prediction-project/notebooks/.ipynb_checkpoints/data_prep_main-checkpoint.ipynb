{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation process for events show\n",
    "==================\n",
    "**Prepared by** : Grej - Mar 11, 2019\n",
    "\n",
    "###### Overview: \n",
    "It is filtered to pre-registrants only.  \n",
    "This means all uploaded files from the organizers were not included.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import geopy.distance\n",
    "import math\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will check the number of registrations per company, this will be used as a new feature.\n",
    "\n",
    "# count the number of registrants per company - categories are combined company name + country, country + website and website only\n",
    "\n",
    "def company_reg_counts(data, data_type):\n",
    "    os.chdir(r'C:\\Users\\User\\Documents\\Data_Science_Projects\\middle-east-event-show-prediction-project')\n",
    "    data_path = data_type+'_raw_data.csv'\n",
    "    # encoding = latin-1 was used here due to the characters that are unreadable when using the standard utf-8\n",
    "    data = pd.read_csv(r'.\\\\data\\\\'+data_path, encoding='latin-1')\n",
    "    data = data[['10 Digit Card Number', 'Company Name', 'Country', 'Website']]\n",
    "    data['country_company'] = data[['Company Name', 'Country']].apply(lambda x: r''.join(x.astype(str)), axis=1).str.lower().str.replace(r' ', r'')\n",
    "    count_per_company = data[data['Company Name']!=' '].country_company.value_counts().rename_axis('x').reset_index(name='count_per_company')\n",
    "    data['country_website'] = data[['Website', 'Country']].apply(lambda x: r''.join(x.astype(str)), axis=1).str.lower().str.replace(r' ', r'')\n",
    "    count_per_comp_website = data[data['Website']!=' '].country_company.value_counts().rename_axis('y').reset_index(name='count_per_comp_website')\n",
    "    count_per_website = data[data['Website']!=' '].Website.value_counts().rename_axis('z').reset_index(name='count_per_website')\n",
    "    \n",
    "    data = data.merge(count_per_company, left_on = 'country_company', right_on = 'x', how = 'left')\n",
    "    data = data.merge(count_per_comp_website, left_on = 'country_website', right_on = 'y', how = 'left')\n",
    "    data = data.merge(count_per_website, left_on = 'Website', right_on = 'z', how = 'left')\n",
    "    company_reg_counts_data = data[['10 Digit Card Number', 'count_per_company', 'count_per_comp_website', 'count_per_website']].fillna(1)\n",
    "    company_reg_counts_data.to_pickle(r'.\\data\\output\\company_reg_count.pkl')\n",
    "    return company_reg_counts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_type):\n",
    "    os.chdir(r'C:\\Users\\User\\Documents\\Data_Science_Projects\\middle-east-event-show-prediction-project')\n",
    "    data_path = data_type+'_raw_data.csv'\n",
    "    # encoding = latin-1 was used here due to the characters that are unreadable when using the standard utf-8\n",
    "    data = pd.read_csv(r'.\\\\data\\\\'+data_path, encoding='latin-1')\n",
    "\n",
    "    # add show columns\n",
    "    data['show'] = 'mese'\n",
    "    data['show_date'] = pd.to_datetime('4/22/2018')\n",
    "\n",
    "    data = data.rename(columns={'Responses':'merged'})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def append_all_files(files):\\n    \\n    adestra = pd.DataFrame()\\n    for file in files:\\n        path = str('.\\\\data\\\\adestra\\\\' + file)\\n        data = pd.read_csv(path)\\n        adestra = adestra.append(data, ignore_index=True)\\n    adestra['opened_email_broadcast'] = 1\\n    adestra = adestra[['email', 'opened_email_broadcast']].rename(columns={'email': 'Email'})\\n    count_number_email_opened = adestra.Email.value_counts().rename_axis('Email').reset_index(name='count_number_email_opened')\\n    adestra = adestra.merge(count_number_email_opened, on='Email', how='left')\\n    adestra = adestra.drop_duplicates(subset=['Email', 'opened_email_broadcast'], keep='first').reset_index().drop('index', axis=True)\\n    adestra.to_pickle(r'.\\\\data\\\\output\\x07destra_data.pkl')\\n    return adestra\""
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def append_all_files(files):\n",
    "    \n",
    "    adestra = pd.DataFrame()\n",
    "    for file in files:\n",
    "        path = str('.\\\\data\\\\adestra\\\\' + file)\n",
    "        data = pd.read_csv(path)\n",
    "        adestra = adestra.append(data, ignore_index=True)\n",
    "    adestra['opened_email_broadcast'] = 1\n",
    "    adestra = adestra[['email', 'opened_email_broadcast']].rename(columns={'email': 'Email'})\n",
    "    count_number_email_opened = adestra.Email.value_counts().rename_axis('Email').reset_index(name='count_number_email_opened')\n",
    "    adestra = adestra.merge(count_number_email_opened, on='Email', how='left')\n",
    "    adestra = adestra.drop_duplicates(subset=['Email', 'opened_email_broadcast'], keep='first').reset_index().drop('index', axis=True)\n",
    "    adestra.to_pickle(r'.\\data\\output\\adestra_data.pkl')\n",
    "    return adestra'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dummify all responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummify_responses(data, codes):\n",
    "    columns_to_check = {'merged', '10 Digit Card Number'}\n",
    "    for cols in columns_to_check:\n",
    "        if cols not in data.columns.values:\n",
    "            print('There is no ',cols,' column')\n",
    "            break\n",
    "    else:\n",
    "        responses = data['merged'].str.split(r']', expand=True)\n",
    "        print('checkpoint 1 -- '+ str(len(responses)))\n",
    "        \n",
    "        responses['10 Digit Card Number'] = data['10 Digit Card Number']\n",
    "        print('checkpoint 2 -- '+ str(len(responses)))\n",
    "        \n",
    "        responses = responses.melt(id_vars=['10 Digit Card Number'], value_name = 'code')\n",
    "        responses['value'] = 1\n",
    "        print('checkpoint 3 -- '+ str(len(responses)))\n",
    "        \n",
    "        responses = responses.merge(codes, left_on = 'code', right_on = 'code', how = 'left')\n",
    "        print('checkpoint 4 -- '+ str(len(responses)))\n",
    "        print(responses.columns)\n",
    "        print(responses['included'].value_counts())\n",
    "        \n",
    "        responses = responses.loc[responses['included'] == 'YES']\n",
    "        print('checkpoint 4.1 -- '+ str(len(responses)))\n",
    "        responses = responses.drop(['show', 'question', 'code', 'code_2', 'text_answer', 'included', 'job_rank'], axis=1)\n",
    "        print('checkpoint 5 -- '+ str(len(responses)))\n",
    "        \n",
    "        responses = responses.pivot_table(index = '10 Digit Card Number', columns = 'decode', values = 'value', aggfunc = 'max')\n",
    "        print('checkpoint 6 -- '+ str(len(responses)))\n",
    "\n",
    "        #responses.loc[responses['Attended']!=1, 'Attended'] = 0\n",
    "        return responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering\n",
    "======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add email and website features\n",
    "This part is a feature engineering process.  \n",
    "The logic behind is that registrants that has entered websites and emails might have correlation to those who attend.  \n",
    "One reason might be because those who have websites and emails are more interested and their company's are active in the industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance of the country from UAE might be also a factor.  \n",
    "The hypothesis is those who come from farther places are less likely to attend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate distance of country from UAE.\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    from math import radians, cos, sin, asin, sqrt\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def with_email(data):\\n    if \\'Email\\' in data.columns.values:\\n        adestra = pd.read_pickle(r\\'.\\\\data\\\\output\\x07destra_data.pkl\\')\\n        data = data.merge(adestra, on=\\'Email\\', how=\\'left\\')\\n        data.loc[data[\\'Email\\']==\" \", \\'with_email\\'] = 0\\n        data.loc[data[\\'Email\\']!=\" \", \\'with_email\\'] = 1\\n        return data\\n    else:\\n        print(\\'There is no Email column\\')\\n'"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def with_website(data):\n",
    "    if 'Website' in data.columns.values:\n",
    "        data.loc[data['Website']==\" \", 'with_website'] = 0\n",
    "        data.loc[data['Website']!=\" \", 'with_website'] = 1\n",
    "        return data\n",
    "    else:\n",
    "        print('There is no website column')\n",
    "\n",
    "'''def with_email(data):\n",
    "    if 'Email' in data.columns.values:\n",
    "        adestra = pd.read_pickle(r'.\\data\\output\\adestra_data.pkl')\n",
    "        data = data.merge(adestra, on='Email', how='left')\n",
    "        data.loc[data['Email']==\" \", 'with_email'] = 0\n",
    "        data.loc[data['Email']!=\" \", 'with_email'] = 1\n",
    "        return data\n",
    "    else:\n",
    "        print('There is no Email column')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add days_to_go and weeks_to_go feature\n",
    "The hypothesis is those who register close to the date of the show are more likely to attend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_to_go_reg(data):\n",
    "    if 'Date Created' in data.columns.values:\n",
    "        data['Date Created'] = pd.to_datetime(data['Date Created'])\n",
    "        difference = data['show_date'] - data['Date Created']\n",
    "        return difference\n",
    "    else:\n",
    "        print('There is no Date Created column')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create groupings for UAE States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_group(data):\n",
    "    if 'State' in data.columns.values:\n",
    "        data['State'] = data['State'].str.lower().str.replace(r' ', r'_')\n",
    "        data.loc[data['country']!= 'United Arab Emirates', 'State'] = 'international_state'\n",
    "        data.loc[data['State']== r' ', 'State'] = 'dubai'\n",
    "        data.loc[data['State']== r'_', 'State'] = 'dubai'\n",
    "        print('\\n State Summary\\n')\n",
    "        print(data['State'].value_counts())\n",
    "        return data\n",
    "    else:\n",
    "        print('There is no State column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleanup the country and group into regions, add the distance of countries from UAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_country(data, region):\n",
    "    if 'Country' in data.columns.values:\n",
    "        data.loc[pd.isnull(data['Country']), 'Country'] = 'United Arab Emirates' # replace blank countries with UAE\n",
    "        data.loc[data['Country']=='', 'Country'] = 'United Arab Emirates' # replace blank countries with UAE\n",
    "        data.loc[data['Country']==' ', 'Country'] = 'United Arab Emirates' # replace blank countries with UAE\n",
    "        \n",
    "        data = data.merge(region, left_on = 'Country', right_on = 'country', how = 'left')\n",
    "        no_region = data.loc[pd.isnull(data['region_2'])]\n",
    "        print('\\n Region Summary\\n')\n",
    "        print(data['region_2'].value_counts())\n",
    "        print('\\nNumber of No Region -- '+ str(len(no_region)))\n",
    "        return data, no_region\n",
    "    else:\n",
    "        print('There is no Country column')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_encode(data, columns):\n",
    "    for cols in columns:\n",
    "        data['mean_encode_'+cols] = data[cols].map(data.groupby(cols)['attended'].mean())\n",
    "        print('\\nMean Encoder Summary -- '+cols)\n",
    "        print(data['mean_encode_'+cols].describe())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dummify all other categorical variables not included in responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummify_columns(data, columns):\n",
    "    columns_to_check = {'10 Digit Card Number'}\n",
    "    for cols in columns_to_check:\n",
    "        if cols not in data.columns.values:\n",
    "            print('There is no ',cols,' column')\n",
    "            break\n",
    "    data_1 = data[columns]\n",
    "    data_1 = pd.get_dummies(data_1[columns])\n",
    "    data_1['10 Digit Card Number'] = data['10 Digit Card Number']\n",
    "    data = data.merge(data_1, on='10 Digit Card Number', how = 'left')\n",
    "    data = data.drop(columns, axis=1)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length is 4860\n",
      "       show                                     question         code code_2  \\\n",
      "0    mese18                                     CATEGORY    mese18VIS    VIS   \n",
      "1    mese18                                     CATEGORY    mese18EXH    EXH   \n",
      "2    mese18                                     CATEGORY    mese18SPK    SPK   \n",
      "3    mese18                                     CATEGORY    mese18MED    MED   \n",
      "4    mese18                                     CATEGORY    mese18VIP    VIP   \n",
      "5    mese18                                     CATEGORY    mese18SPO    SPO   \n",
      "6    mese18                                     CATEGORY    mese18ORG    ORG   \n",
      "7    mese18                         Nature of Business\\t     mese1801     01   \n",
      "8    mese18                         Nature of Business\\t     mese1802     02   \n",
      "9    mese18                         Nature of Business\\t     mese1803     03   \n",
      "10   mese18                         Nature of Business\\t     mese1804     04   \n",
      "11   mese18                         Nature of Business\\t     mese1805     05   \n",
      "12   mese18                         Nature of Business\\t     mese1806     06   \n",
      "13   mese18                         Nature of Business\\t     mese1807     07   \n",
      "14   mese18                         Nature of Business\\t     mese1808     08   \n",
      "15   mese18                         Nature of Business\\t     mese1809     09   \n",
      "16   mese18                         Nature of Business\\t     mese1810     10   \n",
      "17   mese18                         Nature of Business\\t     mese1811     11   \n",
      "18   mese18                         Nature of Business\\t     mese1812     12   \n",
      "19   mese18                         Nature of Business\\t     mese1813     13   \n",
      "20   mese18                         Nature of Business\\t     mese1814     14   \n",
      "21   mese18                         Nature of Business\\t     mese1815     15   \n",
      "22   mese18                         Nature of Business\\t     mese1816     16   \n",
      "23   mese18                         Nature of Business\\t     mese1817     17   \n",
      "24   mese18                         Nature of Business\\t     mese1818     18   \n",
      "25   mese18                         Nature of Business\\t     mese1820     20   \n",
      "26   mese18                             Area of Interest     mese1821     21   \n",
      "27   mese18                             Area of Interest     mese1822     22   \n",
      "28   mese18                             Area of Interest     mese1823     23   \n",
      "29   mese18                             Area of Interest     mese1824     24   \n",
      "..      ...                                          ...          ...    ...   \n",
      "922  mese17             Please indicate Area of Interest     mese1733     33   \n",
      "923  mese17             Please indicate Area of Interest     mese1734     34   \n",
      "924  mese17             Please indicate Area of Interest     mese1735     35   \n",
      "925  mese17             Please indicate Area of Interest     mese1736     36   \n",
      "926  mese17             Please indicate Area of Interest     mese1737     37   \n",
      "927  mese17             Please indicate Area of Interest     mese1738     38   \n",
      "928  mese17             Please indicate Area of Interest     mese1740     40   \n",
      "929  mese17             Please indicate Area of Interest     mese1741     41   \n",
      "930  mese17  Please indicate the Nature of your Business     mese1701     01   \n",
      "931  mese17  Please indicate the Nature of your Business     mese1702     02   \n",
      "932  mese17  Please indicate the Nature of your Business     mese1703     03   \n",
      "933  mese17  Please indicate the Nature of your Business     mese1704     04   \n",
      "934  mese17  Please indicate the Nature of your Business     mese1705     05   \n",
      "935  mese17  Please indicate the Nature of your Business    mese1706A    06A   \n",
      "936  mese17  Please indicate the Nature of your Business    mese1706B    06B   \n",
      "937  mese17  Please indicate the Nature of your Business     mese1707     07   \n",
      "938  mese17  Please indicate the Nature of your Business     mese1708     08   \n",
      "939  mese17  Please indicate the Nature of your Business     mese1709     09   \n",
      "940  mese17  Please indicate the Nature of your Business     mese1710     10   \n",
      "941  mese17  Please indicate the Nature of your Business     mese1711     11   \n",
      "942  mese17  Please indicate the Nature of your Business     mese1713     13   \n",
      "943  mese17  Please indicate the Nature of your Business     mese1715     15   \n",
      "944  mese17  Please indicate the Nature of your Business     mese1716     16   \n",
      "945  mese17  Please indicate the Nature of your Business     mese1718     18   \n",
      "946  mese17  Please indicate the Nature of your Business     mese1719     19   \n",
      "947  mese17                              UTM Source Code  mese17UTMCA  UTMCA   \n",
      "948  mese17                              UTM Source Code  mese17UTMCO  UTMCO   \n",
      "949  mese17                              UTM Source Code  mese17UTMME  UTMME   \n",
      "950  mese17                              UTM Source Code  mese17UTMRE  UTMRE   \n",
      "951  mese17                              UTM Source Code  mese17UTMSO  UTMSO   \n",
      "\n",
      "                                               decode text_answer included  \\\n",
      "0                                             visitor          No      YES   \n",
      "1                                           exhibitor         NaN      YES   \n",
      "2                                             speaker         NaN      YES   \n",
      "3                                               media         NaN      YES   \n",
      "4                                                 vip         NaN      YES   \n",
      "5                                             sponsor         NaN      YES   \n",
      "6                                           organizer         NaN      YES   \n",
      "7                                     event_organiser         NaN      YES   \n",
      "8                             event_management_agency         NaN      YES   \n",
      "9                                conference_organiser         NaN      YES   \n",
      "10                                         exhibition         NaN      YES   \n",
      "11                             concert_event_promoter         NaN      YES   \n",
      "12    marketing_or_procurement_professional_corporate         NaN      YES   \n",
      "13   marketing_or_procurement_professional_government         NaN      YES   \n",
      "14         creative_agency_public_relations_and_sales         NaN      YES   \n",
      "15                           hospitality_and_catering         NaN      YES   \n",
      "16                                        association         NaN      YES   \n",
      "17                     destination_management_company         NaN      YES   \n",
      "18                                      venue_planner         NaN      YES   \n",
      "19                                              media         NaN      YES   \n",
      "20                                technology_provider         NaN      YES   \n",
      "21                               registration_company         NaN      YES   \n",
      "22                                   event_production         NaN      YES   \n",
      "23                    project_management_and_planning         NaN      YES   \n",
      "24                             entertainment_provider         NaN      YES   \n",
      "25                     other_services_please_specify_         NaN      YES   \n",
      "26                                      av_production         NaN      YES   \n",
      "27                       brand_communication_agencies         NaN      YES   \n",
      "28                                   design_agencies_         NaN      YES   \n",
      "29                                      entertainment         NaN      YES   \n",
      "..                                                ...         ...      ...   \n",
      "922                           promotional_merchandise         NaN      YES   \n",
      "923                                         ticketing         NaN      YES   \n",
      "924                                          staffing         NaN      YES   \n",
      "925                               content_development         NaN      YES   \n",
      "926                               corporate_marketing         NaN      YES   \n",
      "927                                  event_management         NaN      YES   \n",
      "928                       videography_and_photography         NaN      YES   \n",
      "929                     other_servces_please_specify_         NaN      YES   \n",
      "930                                   event_organiser         NaN      YES   \n",
      "931                           event_management_agency         NaN      YES   \n",
      "932                              conference_organiser         NaN      YES   \n",
      "933                                        exhibition         NaN      YES   \n",
      "934                            concert_event_promoter         NaN      YES   \n",
      "935   marketing_or_procurement_professional_corporate         NaN      YES   \n",
      "936  marketing_or_procurement_professional_government         NaN      YES   \n",
      "937        creative_agency_public_relations_and_sales         NaN      YES   \n",
      "938                          hospitality_and_catering         NaN      YES   \n",
      "939                                       association         NaN      YES   \n",
      "940                    destination_management_company         NaN      YES   \n",
      "941                                     venue_planner         NaN      YES   \n",
      "942                                             media         NaN      YES   \n",
      "943                              registration_company         NaN      YES   \n",
      "944                                  event_production         NaN      YES   \n",
      "945                   project_management_and_planning         NaN      YES   \n",
      "946                    other_services_please_specify_         NaN      YES   \n",
      "947                                      utm_campaign         NaN      YES   \n",
      "948                                       utm_content         NaN      YES   \n",
      "949                                        utm_medium         NaN      YES   \n",
      "950                                       utm_refcode         NaN      YES   \n",
      "951                                        utm_source         NaN      YES   \n",
      "\n",
      "     job_rank  \n",
      "0         NaN  \n",
      "1         NaN  \n",
      "2         NaN  \n",
      "3         NaN  \n",
      "4         NaN  \n",
      "5         NaN  \n",
      "6         NaN  \n",
      "7         NaN  \n",
      "8         NaN  \n",
      "9         NaN  \n",
      "10        NaN  \n",
      "11        NaN  \n",
      "12        NaN  \n",
      "13        NaN  \n",
      "14        NaN  \n",
      "15        NaN  \n",
      "16        NaN  \n",
      "17        NaN  \n",
      "18        NaN  \n",
      "19        NaN  \n",
      "20        NaN  \n",
      "21        NaN  \n",
      "22        NaN  \n",
      "23        NaN  \n",
      "24        NaN  \n",
      "25        NaN  \n",
      "26        NaN  \n",
      "27        NaN  \n",
      "28        NaN  \n",
      "29        NaN  \n",
      "..        ...  \n",
      "922       NaN  \n",
      "923       NaN  \n",
      "924       NaN  \n",
      "925       NaN  \n",
      "926       NaN  \n",
      "927       NaN  \n",
      "928       NaN  \n",
      "929       NaN  \n",
      "930       NaN  \n",
      "931       NaN  \n",
      "932       NaN  \n",
      "933       NaN  \n",
      "934       NaN  \n",
      "935       NaN  \n",
      "936       NaN  \n",
      "937       NaN  \n",
      "938       NaN  \n",
      "939       NaN  \n",
      "940       NaN  \n",
      "941       NaN  \n",
      "942       NaN  \n",
      "943       NaN  \n",
      "944       NaN  \n",
      "945       NaN  \n",
      "946       NaN  \n",
      "947       NaN  \n",
      "948       NaN  \n",
      "949       NaN  \n",
      "950       NaN  \n",
      "951       NaN  \n",
      "\n",
      "[952 rows x 8 columns]\n",
      "checkpoint 1 -- 4860\n",
      "checkpoint 2 -- 4860\n",
      "checkpoint 3 -- 364500\n",
      "checkpoint 4 -- 364500\n",
      "Index(['10 Digit Card Number', 'variable', 'code', 'value', 'show', 'question',\n",
      "       'code_2', 'decode', 'text_answer', 'included', 'job_rank'],\n",
      "      dtype='object')\n",
      "Series([], Name: included, dtype: int64)\n",
      "checkpoint 4.1 -- 0\n",
      "checkpoint 5 -- 0\n",
      "checkpoint 6 -- 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:544: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Region Summary\n",
      "\n",
      "ME GCC            4568\n",
      "Europe              93\n",
      "Australia-Asia      66\n",
      "Africa              62\n",
      "Americas            44\n",
      "ME Non-GCC          26\n",
      "Name: region_2, dtype: int64\n",
      "\n",
      "Number of No Region -- 1\n",
      "\n",
      " State Summary\n",
      "\n",
      "dubai                  3766\n",
      "international_state     440\n",
      "abu_dhabi               404\n",
      "sharjah                 165\n",
      "ras_al_khaimah           40\n",
      "fujairah                 24\n",
      "ajman                    21\n",
      "Name: State, dtype: int64\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Column not found: attended'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-351-db43911159f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m# create mean encoded features for ['State', 'region_1', 'region_2']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'State'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'country'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'region_1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'region_2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'days_to_go'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'State'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'region_1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'region_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-349-69714b0f7b1f>\u001b[0m in \u001b[0;36mmean_encode\u001b[1;34m(data, columns)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmean_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcols\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_encode_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'attended'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nMean Encoder Summary -- '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_encode_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Column not found: {key}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gotitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Column not found: attended'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # type of data to be processed\n",
    "    data_type = 'train'\n",
    "    \n",
    "\n",
    "    '''# open adestra files\n",
    "    files = os.listdir(r'.\\data\\adestra')\n",
    "    adestra = append_all_files(files)'''\n",
    "    \n",
    "    # load data\n",
    "    data = load_data(data_type)\n",
    "    \n",
    "    company_reg_counts_data = company_reg_counts(data, data_type)\n",
    "    \n",
    "    data = data.merge(company_reg_counts_data,  on='10 Digit Card Number', how = 'left')\n",
    "    data['company_name'] = data[['Company Name']].apply(lambda x: r''.join(x.astype(str)), axis=1).str.lower().str.replace(r' ', r'')\n",
    "    \n",
    "    print('data length is '+ str(len(data)))\n",
    "    # Load the codes data and one-hot-code all responses\n",
    "    codes = pd.read_excel(r'.\\data\\codes.xlsx')\n",
    "    print(codes)\n",
    "    responses = dummify_responses(data, codes)\n",
    "    data = data.merge(responses, on='10 Digit Card Number', how = 'left').drop('merged', axis=1)\n",
    "    data = data.fillna(0)\n",
    "\n",
    "    # add with_website, with_email, days_to_go, weeks_to_go feature\n",
    "    data = with_website(data)\n",
    "    ##data = with_email(data)\n",
    "    data = data.drop(['Email', 'Website'], axis=1)\n",
    "    difference = days_to_go_reg(data)\n",
    "    data['days_to_go'] = difference.dt.days\n",
    "    data['weeks_to_go'] = round(data['days_to_go']/7)\n",
    "    data = data.drop(['Date Created', 'show_date'], axis=1)\n",
    "\n",
    " \n",
    "    # Load regions data and cleanup the country\n",
    "    region = pd.read_excel(r'.\\data\\region.xlsx')\n",
    "    data, no_region = cleanup_country(data, region)\n",
    "        \n",
    "    # create the State binning feature\n",
    "    data = create_state_group(data)\n",
    "\n",
    "    # create mean encoded features for ['State', 'region_1', 'region_2']\n",
    "    \n",
    "    data = mean_encode(data, columns = ['State', 'country', 'region_1', 'region_2', 'days_to_go'])\n",
    "  \n",
    "    columns = ['State', 'region_1', 'region_2']\n",
    "    data = dummify_columns(data, columns)\n",
    "    data = data.fillna(0)\n",
    "    \n",
    "    # add distance feature using latitude and longitude\n",
    "    distance = pd.Series([])\n",
    "    for i in range(len(data.index)):\n",
    "        lon1 = 53.847818\n",
    "        lat1 = 23.424076\n",
    "        lon2 = data.loc[i,['longitude']]\n",
    "        lat2 = data.loc[i,['latitude']]\n",
    "        dist = pd.Series(haversine(lon1, lat1, lon2, lat2))\n",
    "        distance = distance.append(dist, ignore_index=True)\n",
    "    data['distance'] = distance\n",
    "    \n",
    "    # below are situation-based wrangling of data - \n",
    "    # removed and filtered some features that are not used anymore\n",
    "    \n",
    "    data = data.drop(['country', 'latitude', 'longitude', 'with_website', 'show', 'Country', 'company_name'], axis=True)\n",
    "    data = data.drop('Company Name', axis=True)\n",
    "    data = data[data['days_to_go']>=1].reset_index().drop('index', axis=1)\n",
    "    \n",
    "    data = data.rename(columns={'attended': 'target', '10 Digit Card Number':'card_number'})\n",
    "    \n",
    "\n",
    "    # saving the data as cleanData\n",
    "    data.to_csv(r'.\\data\\output\\train_clean_data.csv', index=False)\n",
    "    \n",
    "    # identify the features - important to have the same feature for the test Data\n",
    "    columns = pd.DataFrame({'cols' : data.columns})\n",
    "    columns = columns[1:]\n",
    "    columns = columns[columns['cols']!='target']\n",
    "    columns.to_csv(r'.\\data\\output\\columns_used_for_model.csv', index=False)\n",
    "    print('\\nNumber of Features -- '+ str(len(columns)))\n",
    "    \n",
    "    print('\\nDone...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
