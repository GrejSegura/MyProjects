{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting new observations using pytorch model\n",
    "=========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "\n",
    "## clear GPU memory to optimize capacity\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "os.chdir(r'C:\\Users\\User\\Documents\\Data_Science_Projects\\food-expo-attendee-prediction-project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-process the data to match the requirement of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# encoding = latin-1 was used here due to the characters that are unreadable when using the standard utf-8\n",
    "data = pd.read_csv(r'.\\data\\gfd2019.csv', encoding='latin-1')\n",
    "\n",
    "# add show columns\n",
    "data['show'] = 'gfd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             1/19/2019\n",
       "1             1/19/2019\n",
       "2             1/19/2019\n",
       "3        2/11/2019 0:00\n",
       "4        2/11/2019 0:00\n",
       "5        2/11/2019 0:00\n",
       "6             2/19/2019\n",
       "7             1/27/2019\n",
       "8             1/27/2019\n",
       "9        1/11/2019 0:00\n",
       "10       1/11/2019 0:00\n",
       "11       1/11/2019 0:00\n",
       "12       1/11/2019 0:00\n",
       "13       1/11/2019 0:00\n",
       "14       1/11/2019 0:00\n",
       "15       1/11/2019 0:00\n",
       "16       1/11/2019 0:00\n",
       "17       1/11/2019 0:00\n",
       "18       1/11/2019 0:00\n",
       "19       1/11/2019 0:00\n",
       "20       1/11/2019 0:00\n",
       "21       1/11/2019 0:00\n",
       "22       1/11/2019 0:00\n",
       "23       1/11/2019 0:00\n",
       "24       1/11/2019 0:00\n",
       "25       1/11/2019 0:00\n",
       "26       1/11/2019 0:00\n",
       "27       1/11/2019 0:00\n",
       "28       1/11/2019 0:00\n",
       "29       1/11/2019 0:00\n",
       "              ...      \n",
       "70014         1/21/2019\n",
       "70015         1/21/2019\n",
       "70016         1/21/2019\n",
       "70017         1/21/2019\n",
       "70018         1/21/2019\n",
       "70019     2/8/2019 0:00\n",
       "70020     2/8/2019 0:00\n",
       "70021     1/9/2019 0:00\n",
       "70022    2/11/2019 0:00\n",
       "70023         1/18/2019\n",
       "70024         2/19/2019\n",
       "70025     2/6/2019 0:00\n",
       "70026     1/8/2019 0:00\n",
       "70027     1/8/2019 0:00\n",
       "70028     1/3/2019 0:00\n",
       "70029     2/6/2019 0:00\n",
       "70030         1/22/2019\n",
       "70031     2/5/2019 0:00\n",
       "70032     2/5/2019 0:00\n",
       "70033     2/8/2019 0:00\n",
       "70034         2/16/2019\n",
       "70035     2/3/2019 0:00\n",
       "70036        12/27/2018\n",
       "70037        12/27/2018\n",
       "70038     2/7/2019 0:00\n",
       "70039     2/7/2019 0:00\n",
       "70040         2/19/2019\n",
       "70041    1/11/2019 0:00\n",
       "70042    1/11/2019 0:00\n",
       "70043    1/11/2019 0:00\n",
       "Name: Date Created, Length: 70044, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Date Created']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2019-01-19\n",
       "1       2019-01-19\n",
       "2       2019-01-19\n",
       "3       2019-02-11\n",
       "4       2019-02-11\n",
       "5       2019-02-11\n",
       "6       2019-02-19\n",
       "7       2019-01-27\n",
       "8       2019-01-27\n",
       "9       2019-01-11\n",
       "10      2019-01-11\n",
       "11      2019-01-11\n",
       "12      2019-01-11\n",
       "13      2019-01-11\n",
       "14      2019-01-11\n",
       "15      2019-01-11\n",
       "16      2019-01-11\n",
       "17      2019-01-11\n",
       "18      2019-01-11\n",
       "19      2019-01-11\n",
       "20      2019-01-11\n",
       "21      2019-01-11\n",
       "22      2019-01-11\n",
       "23      2019-01-11\n",
       "24      2019-01-11\n",
       "25      2019-01-11\n",
       "26      2019-01-11\n",
       "27      2019-01-11\n",
       "28      2019-01-11\n",
       "29      2019-01-11\n",
       "           ...    \n",
       "70014   2019-01-21\n",
       "70015   2019-01-21\n",
       "70016   2019-01-21\n",
       "70017   2019-01-21\n",
       "70018   2019-01-21\n",
       "70019   2019-02-08\n",
       "70020   2019-02-08\n",
       "70021   2019-01-09\n",
       "70022   2019-02-11\n",
       "70023   2019-01-18\n",
       "70024   2019-02-19\n",
       "70025   2019-02-06\n",
       "70026   2019-01-08\n",
       "70027   2019-01-08\n",
       "70028   2019-01-03\n",
       "70029   2019-02-06\n",
       "70030   2019-01-22\n",
       "70031   2019-02-05\n",
       "70032   2019-02-05\n",
       "70033   2019-02-08\n",
       "70034   2019-02-16\n",
       "70035   2019-02-03\n",
       "70036   2018-12-27\n",
       "70037   2018-12-27\n",
       "70038   2019-02-07\n",
       "70039   2019-02-07\n",
       "70040   2019-02-19\n",
       "70041   2019-01-11\n",
       "70042   2019-01-11\n",
       "70043   2019-01-11\n",
       "Name: Date Created, Length: 70044, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(data['Date Created'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "## merge and drop columns\n",
    "def pre_process(data, merge_columns, show_date): # merge_columns and drop_columns are relative to the show\n",
    "    data['10 Digit Card Number'] = pd.to_numeric(data['10 Digit Card Number'])\n",
    "    data['merged'] = data[merge_columns].apply(lambda x: ']' + x.astype(str), axis=1).apply(lambda x: r''.join(x.astype(str)), axis=1).str.replace(r' ', r'')\n",
    "    data = data[['10 Digit Card Number', 'State', 'Country', 'Date Created', 'Email',\n",
    "       'Website', 'show', 'merged']]\n",
    "    data['show_date'] = pd.to_datetime(show_date)\n",
    "    return data\n",
    "\n",
    "# merge and drop for GFD\n",
    "merge_columns = ['CATEGORY',\n",
    "       'Company Industry*', 'Job Function*', 'Company\\'s Main Activity',\n",
    "       'Company\\'s size (No. of employees)*', 'Job/Purchasing Role*',\n",
    "       'I am interested in the following food/beverage products',\n",
    "       'I am interested in the following food/beverage products - BAKERY',\n",
    "       'I am interested in the following food/beverage products - BEVERAGES',\n",
    "       'I am interested in the following food/beverage products - CHILLED & FRESH FOOD\\r\\n',\n",
    "       'I am interested in the following food/beverage products - CONFECTIONARY\\r\\n',\n",
    "       'I am interested in the following food/beverage products - DAIRY\\r\\n',\n",
    "       'I am interested in the following food/beverage products - FROZEN FOOD',\n",
    "       'I am interested in the following food/beverage products - GRAINS / CEREALS / FLOURS\\r\\n',\n",
    "       'I am interested in the following food/beverage products - HEALTH, WELLNESS & FREE-FROM PRODUCTS\\r\\n',\n",
    "       'I am interested in the following food/beverage products - MEAT & POULTRY\\r\\n',\n",
    "       'I am interested in the following food/beverage products - SEAFOOD\\r\\n',\n",
    "       'I am interested in the following food/beverage products - SNACKS ',\n",
    "       'I am interested in the following food/beverage products - SPECIALITY FOOD',\n",
    "       'I am interested in the following food/beverage products - FEDERAL GOVERNMENT',\n",
    "       'I am interested in the following food/beverage products - LOCAL GOVERNMENT\\r\\n',\n",
    "       'I am interested in the following food/beverage products - NON-PROFIT\\r\\n',\n",
    "       'I am interested in the following food/beverage products - PROFESSIONAL SERVICES\\r\\n',\n",
    "       'I am interested to attend the Gulfood Innovation Summit? ',\n",
    "       'I want to receive a free ticket to attend PRIME - Private Label & Licensing Middle East (29-31 OCT 2019 | Dubai)',\n",
    "       'Exhibitor I-Invite Product Sector', 'Gender', 'Terms & Conditions',\n",
    "       'REGISTRATION TYPE', 'NATIONALITY - DWTC', 'Country of Company',\n",
    "       'Language Code', 'UPLOAD CODES', 'Email Broadcast', 'PROMO CODE',\n",
    "       'DWTC Booking Platform', 'Exhibition Packages', 'Payment Stages',\n",
    "       'PAYMENT CHOICE', 'PAYMENT STATUS', 'PAYMENT MODE', 'PAYMENT DETAILS ',\n",
    "       'UTM Values', 'DATABASE MANAGEMENT', 'DROP OFF MANAGEMENT',\n",
    "       'DTCM - DEFAULT CODES', 'DTCM Basket Codes', 'Exhibitor Platform',\n",
    "       'API Codes', 'AGE GROUP - Exhibitor',\n",
    "       'Attendance Status (Telemarketing Codes)',\n",
    "       'Reason for not Attending (Telemarketing Codes)',\n",
    "       'Registration status (Telemarketing Codes)',\n",
    "       'Actual Registration status (Telemarketing Codes) ***Automated - Please don\\'t update!!!',\n",
    "       'Exhibitor I-Invite (Code)', 'Recommend a Colleague Codes',\n",
    "       'MAJLIS Management', 'MAJLIS Nominating Company']\n",
    "\n",
    "data = pre_process(data, merge_columns=merge_columns, show_date='2019-02-17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def replace_codes(data, show):\n",
    "    data['merged'] = r']'+data['merged']\n",
    "    data['merged'] = data['merged'].str.replace(r']', r']' + show)\n",
    "    return data\n",
    "data = replace_codes(data, show='GFD19')'''\n",
    "\n",
    "def with_website(data):\n",
    "    if 'Website' in data.columns.values:\n",
    "        data.loc[data['Website']==\" \", 'with_website'] = 0\n",
    "        data.loc[data['Website']!=\" \", 'with_website'] = 1\n",
    "        return data\n",
    "    else:\n",
    "        print('There is no website column')\n",
    "\n",
    "def with_email(data):\n",
    "    if 'Email' in data.columns.values:\n",
    "        data.loc[data['Email']==\" \", 'with_email'] = 0\n",
    "        data.loc[data['Email']!=\" \", 'with_email'] = 1\n",
    "        return data\n",
    "    else:\n",
    "        print('There is no Email column')\n",
    "\n",
    "data = with_website(data)\n",
    "#data = with_email(data)\n",
    "data = data.drop(['Email', 'Website'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_to_go_reg(data):\n",
    "    if 'Date Created' in data.columns.values:\n",
    "        data['Date Created'] = pd.to_datetime(data['Date Created'])\n",
    "        difference = data['show_date'] - data['Date Created']\n",
    "        return difference\n",
    "    else:\n",
    "        print('There is no Date Created column')\n",
    "\n",
    "difference = days_to_go_reg(data)\n",
    "data['days_to_go'] = difference.dt.days\n",
    "data['weeks_to_go'] = round(data['days_to_go']/7)\n",
    "data = data.drop(['Date Created', 'show_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load regions data\n",
    "region = pd.read_excel(r'.\\data\\region.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_country(data, region):\n",
    "    if 'Country' in data.columns.values:\n",
    "        data.loc[pd.isnull(data['Country']), 'Country'] = 'United Arab Emirates' # replace blank countries with UAE\n",
    "        data.loc[data['Country']=='', 'Country'] = 'United Arab Emirates' # replace blank countries with UAE\n",
    "        data.loc[data['Country']==' ', 'Country'] = 'United Arab Emirates' # replace blank countries with UAE\n",
    "        \n",
    "        data = data.merge(region, left_on = 'Country', right_on = 'country', how = 'left')\n",
    "        no_region = data.loc[pd.isnull(data['region_2'])]\n",
    "        data = data.drop('Country', axis=1)\n",
    "        return data, no_region\n",
    "    else:\n",
    "        print('There is no Country column')\n",
    "\n",
    "data, no_region = cleanup_country(data, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r\n",
    "\n",
    "distance = pd.Series([])\n",
    "for i in range(len(data.index)):\n",
    "    lon1 = 53.847818\n",
    "    lat1 = 23.424076\n",
    "    lon2 = data.loc[i,['longitude']]\n",
    "    lat2 = data.loc[i,['latitude']]\n",
    "    dist = pd.Series(haversine(lon1, lat1, lon2, lat2))\n",
    "    distance = distance.append(dist, ignore_index=True)\n",
    "    \n",
    "data['distance'] = distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_group(data):\n",
    "    if 'State' in data.columns.values:\n",
    "        data['State'] = data['State'].str.lower().str.replace(r' ', r'_')\n",
    "        data.loc[data['country']!= 'United Arab Emirates', 'State'] = 'international_state'\n",
    "        data.loc[data['State']== r' ', 'State'] = 'dubai'\n",
    "        data.loc[data['State']== r'_', 'State'] = 'dubai'\n",
    "        data.loc[data['State']== r'', 'State'] = 'dubai'\n",
    "        return data\n",
    "    else:\n",
    "        print('There is no Country column')\n",
    "data_1 = create_state_group(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the codes data\n",
    "codes = pd.read_excel(r'.\\data\\codes.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummify_responses(data, codes):\n",
    "    columns_to_check = {'merged', '10 Digit Card Number'}\n",
    "    for cols in columns_to_check:\n",
    "        if cols not in data.columns.values:\n",
    "            print('There is no ',cols,' column')\n",
    "            break\n",
    "    else:\n",
    "        responses = data['merged'].str.split(r']', expand=True)\n",
    "        responses['10 Digit Card Number'] = data['10 Digit Card Number']\n",
    "        responses = responses.melt(id_vars=['10 Digit Card Number'], value_name = 'code')\n",
    "        responses['value'] = 1\n",
    "        \n",
    "        responses = responses.merge(codes, left_on = 'code', right_on = 'code', how = 'left')\n",
    "        responses = responses.loc[responses['included'] == 'Y']\n",
    "        responses = responses.drop(['show', 'question', 'code', 'text_answer', 'included'], axis=1)\n",
    "        responses = responses.pivot_table(index = '10 Digit Card Number', columns = 'decode', values = 'value', aggfunc = 'max')\n",
    "        #responses.loc[responses['Attended']!=1, 'Attended'] = 0\n",
    "        return responses\n",
    "\n",
    "responses = dummify_responses(data_1, codes)\n",
    "data_1 = data_1.merge(responses, on='10 Digit Card Number', how = 'left').drop('merged', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummify_columns(data, columns):\n",
    "    columns_to_check = {'10 Digit Card Number'}\n",
    "    for cols in columns_to_check:\n",
    "        if cols not in data.columns.values:\n",
    "            print('There is no ',cols,' column')\n",
    "            break\n",
    "    data_1 = data[columns]\n",
    "    data_1 = pd.get_dummies(data_1[columns])\n",
    "    data_1['10 Digit Card Number'] = data['10 Digit Card Number']\n",
    "    data = data.merge(data_1, on='10 Digit Card Number', how = 'left')\n",
    "    data = data.drop(columns, axis=1)\n",
    "    return data\n",
    "\n",
    "#data = data.drop('country', axis=True)\n",
    "columns = ['State', 'country', 'region_1', 'region_2']\n",
    "data_1 = dummify_columns(data_1, columns)\n",
    "data_1 = data_1.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "## count registrants per company\n",
    "gfd = pd.read_csv(r'.\\data\\gfd2019.csv', encoding='latin-1')\n",
    "gfd = gfd[['10 Digit Card Number', 'Company Name', 'Country', 'Website']]\n",
    "\n",
    "def company_reg_counts(data):\n",
    "    data['country_company'] = data[['Company Name', 'Country']].apply(lambda x: r''.join(x.astype(str)), axis=1).str.lower().str.replace(r' ', r'')\n",
    "    count_per_company = data[data['Company Name']!=' '].country_company.value_counts().rename_axis('x').reset_index(name='count_per_company')\n",
    "    data['country_website'] = data[['Website', 'Country']].apply(lambda x: r''.join(x.astype(str)), axis=1).str.lower().str.replace(r' ', r'')\n",
    "    count_per_comp_website = data[data['Website']!=' '].country_company.value_counts().rename_axis('y').reset_index(name='count_per_comp_website')\n",
    "    count_per_website = data[data['Website']!=' '].Website.value_counts().rename_axis('z').reset_index(name='count_per_website')\n",
    "    data = data.merge(count_per_company, left_on = 'country_company', right_on = 'x', how = 'left')\n",
    "    data = data.merge(count_per_comp_website, left_on = 'country_website', right_on = 'y', how = 'left')\n",
    "    data = data.merge(count_per_website, left_on = 'Website', right_on = 'z', how = 'left')\n",
    "    data = data[['10 Digit Card Number', 'count_per_company', 'count_per_comp_website', 'count_per_website']].fillna(1)\n",
    "    return data\n",
    "\n",
    "gfd_count = company_reg_counts(gfd)\n",
    "\n",
    "data_1 = data_1.merge(gfd_count, on='10 Digit Card Number', how='left')\n",
    "ids = data_1['10 Digit Card Number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def preprocess_data(data):\\n    data = data.drop('show', axis=True)\\n    summary = data.describe().transpose()\\n    cols = data[summary[summary['max']>1].reset_index()['index'].tolist()]\\n    cols = cols.columns[1:]\\n\\n    # First, scale the Data - only those numerical/non-categorical\\n    names = data.columns\\n    scaler = preprocessing.StandardScaler()\\n    # Fit your data on the scaler object\\n    scaled_data = scaler.fit_transform(data)\\n    scaled_data = pd.DataFrame(scaled_data, columns=names)\\n    scaled_data = scaled_data[cols] ###------------------->> cols are non-categorical columns\\n    data = data.drop(cols, axis=1)\\n    data = pd.concat([scaled_data, data], axis=1)\\n    return data\\ndata = preprocess_data(data)\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def preprocess_data(data):\n",
    "    data = data.drop('show', axis=True)\n",
    "    summary = data.describe().transpose()\n",
    "    cols = data[summary[summary['max']>1].reset_index()['index'].tolist()]\n",
    "    cols = cols.columns[1:]\n",
    "\n",
    "    # First, scale the Data - only those numerical/non-categorical\n",
    "    names = data.columns\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    # Fit your data on the scaler object\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    scaled_data = pd.DataFrame(scaled_data, columns=names)\n",
    "    scaled_data = scaled_data[cols] ###------------------->> cols are non-categorical columns\n",
    "    data = data.drop(cols, axis=1)\n",
    "    data = pd.concat([scaled_data, data], axis=1)\n",
    "    return data\n",
    "data = preprocess_data(data)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_columns(data):\n",
    "    columns_1 = pd.read_pickle(r'.\\data\\output\\columns_used_for_model.pkl')\n",
    "    columns_1 = np.array(columns_1['cols'])\n",
    "    columns_2 = data.columns\n",
    "    newcols = list(set(columns_1) - set(columns_2))\n",
    "    for cols in newcols:\n",
    "        data[cols] = 0\n",
    "    delCols = list(set(columns_2) - set(columns_1))\n",
    "    data = data.drop(delCols, axis=1)\n",
    "    return data\n",
    "data_1 = match_columns(data_1)\n",
    "data_1['10 Digit Card Number'] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data):\n",
    "    cols = pd.read_csv(r'.\\data\\output\\mean_std_scaler.csv')\n",
    "    for i in range(len(cols)):\n",
    "        column = cols['feature'][i]\n",
    "        mean = cols['mean'][i]\n",
    "        std = cols['std'][i]\n",
    "        data[column] = (data[column]-mean)/std\n",
    "    return data\n",
    "\n",
    "data_1 = scale_data(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.to_pickle(r'.\\data\\output\\dataForPreds.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['show', 'with_website', 'days_to_go', 'weeks_to_go', 'latitude',\n",
       "       'longitude', 'distance', 'arabic_page', 'bakery', 'bar_cafe_restaurant',\n",
       "       ...\n",
       "       'country_Svalbard And Jan Mayen Islands', 'country_Taiwan', 'exhibitor',\n",
       "       'country_British Indian Ocean Territory', 'country_Guam', 'female',\n",
       "       'country_Tonga', 'male', 'warehousing_distribution',\n",
       "       '10 Digit Card Number'],\n",
       "      dtype='object', length=278)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data_1.describe().stack()['max']\n",
    "a = pd.DataFrame(a)\n",
    "a.to_csv(r'.\\data\\output\\a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        29\n",
       "1        29\n",
       "2        29\n",
       "3         6\n",
       "4         6\n",
       "5         6\n",
       "6        -2\n",
       "7        21\n",
       "8        21\n",
       "9        37\n",
       "10       37\n",
       "11       37\n",
       "12       37\n",
       "13       37\n",
       "14       37\n",
       "15       37\n",
       "16       37\n",
       "17       37\n",
       "18       37\n",
       "19       37\n",
       "20       37\n",
       "21       37\n",
       "22       37\n",
       "23       37\n",
       "24       37\n",
       "25       37\n",
       "26       37\n",
       "27       37\n",
       "28       37\n",
       "29       37\n",
       "         ..\n",
       "70122    27\n",
       "70123    27\n",
       "70124    27\n",
       "70125    27\n",
       "70126    27\n",
       "70127     9\n",
       "70128     9\n",
       "70129    39\n",
       "70130     6\n",
       "70131    30\n",
       "70132    -2\n",
       "70133    11\n",
       "70134    40\n",
       "70135    40\n",
       "70136    45\n",
       "70137    11\n",
       "70138    26\n",
       "70139    12\n",
       "70140    12\n",
       "70141     9\n",
       "70142     1\n",
       "70143    14\n",
       "70144    52\n",
       "70145    52\n",
       "70146    10\n",
       "70147    10\n",
       "70148    -2\n",
       "70149    37\n",
       "70150    37\n",
       "70151    37\n",
       "Name: days_to_go, Length: 70152, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1['days_to_go']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
