dtaMining2 <- dtaMining
dtaMining <- dtaMining2[, -c("value")]
dtaMining$value <- dtaMining$value.1
dtaMining <- dtaMining[, -c("value.1")]
# convert to wide
dtaMining <- dcast(dtaMining, id + label + Region.2 + daysToShow ~ value)
dtaMining <- dtaMining[, -1]
dummy <- dummyVars(~., dtaMining)
dummy_dta <- predict(dummy, dtaMining)
dtaMining2 <- as.data.frame(dummy_dta)
dtaMining <- dtaMining2
varnames1 <- grep("attd", names(dtaMining), value = TRUE)
#dtaMining <- dtaMining[, -as.vector(varnames1), with = FALSE]
dtaMining <- dtaMining[, -(max(length(names(dtaMining))))]
dtaMining <- dtaMining[, -c(1, 9)]
write.csv(dtaMining, "./dta/dtaminingClean_for_logisticRegression.csv", row.names = FALSE)
##############################################################################################################################
##############################################################################################################################
##############################################################################################################################
## PART 2 - LOGISTIC REGRESSION
rm(list = ls())
dta <- read.csv("./dta/dtaminingClean_for_logisticRegression.csv")
names(dta)[1] <- "label"
logitModel_1 <- glm(label ~., dta, family = binomial(link = "logit"))
important_vars <- varImp(logitModel_1)
## significant vars only
significantVars <- coef(summary(logitModel_1))
significantVars <- as.data.frame(significantVars)
significantVars <- significantVars[significantVars[, "Pr(>|z|)"] < 0.1, ]
write.csv(significantVars, "./dta/SignificantVariables.csv")
## only the significant variables will be used in the next models
significantVars
rm(list = ls())
library(xgboost)
library(randomForest)
library(caret)
library(data.table)
library(lubridate)
library(ggplot2)
library(DiagrammeR)
## THIS IS AN XGBOOST TRAINER USING THE LOGISTIC REGRESSION FILE
dtaMining <- fread("./dta/dtaminingClean_for_logisticRegression.csv", sep = ",")
sigVars <- read.csv("./dta/SignificantVariables.csv")
sigVars <- sigVars[, 1]
sigVars <- as.vector(sigVars)
names(dtaMining)[1] <- "label"
dtaMining <- dtaMining[, c("label", sigVars), with = FALSE]
d <- 1:nrow(dtaMining)
index <- sample(d, round(nrow(dtaMining)*.8))
#dtaMining[] <- lapply(dtaMining, function(x) as.numeric(x))
train_dta <- dtaMining[index, ]
test_dta <- dtaMining[-index, ]
write.csv(train_dta, "./dta/train.csv", row.names = FALSE)
write.csv(test_dta, "./dta/test.csv", row.names = FALSE)
## XGBOOST ---
train_1 <- train_dta[,2:length(dtaMining)]
test_1 <- test_dta[,2:length(dtaMining)]
train_2 <- train_dta[, 1]
test_2 <- test_dta[, 1]
length(train_2)
train_1[] <- lapply(train_1, function(x) as.numeric(x))
train_2 <- as.numeric(train_2)
test_1[] <- lapply(test_1, function(x) as.numeric(x))
test_2 <- as.numeric(test_2)
xgModel <- xgboost(as.matrix(train_1), as.matrix(train_2),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 2,
max.depth = 8,
eta = 0.01,
nthread = 8,
nrounds = 1200,
min_child_weight = 1,
subsample = .75,
colsample_bytree = .8,
num_parallel_tree = 1)
rm(list = ls())
library(xgboost)
library(randomForest)
library(caret)
library(data.table)
library(lubridate)
library(ggplot2)
library(DiagrammeR)
## THIS IS AN XGBOOST TRAINER USING THE LOGISTIC REGRESSION FILE
dtaMining <- fread("./dta/dtaminingClean_for_logisticRegression.csv", sep = ",")
sigVars <- read.csv("./dta/SignificantVariables.csv")
sigVars <- sigVars[, 1]
sigVars <- as.vector(sigVars)
names(dtaMining)[1] <- "label"
dtaMining <- dtaMining[, c("label", sigVars), with = FALSE]
d <- 1:nrow(dtaMining)
index <- sample(d, round(nrow(dtaMining)*.8))
#dtaMining[] <- lapply(dtaMining, function(x) as.numeric(x))
train_dta <- dtaMining[index, ]
test_dta <- dtaMining[-index, ]
write.csv(train_dta, "./dta/train.csv", row.names = FALSE)
write.csv(test_dta, "./dta/test.csv", row.names = FALSE)
## XGBOOST ---
train_1 <- train_dta[,2:length(dtaMining)]
test_1 <- test_dta[,2:length(dtaMining)]
train_2 <- train_dta[, 1]
test_2 <- test_dta[, 1]
length(train_2)
train_1[] <- lapply(train_1, function(x) as.numeric(x))
train_2 <- as.numeric(train_2)
test_1[] <- lapply(test_1, function(x) as.numeric(x))
test_2 <- as.numeric(test_2)
xgModel <- xgboost(as.matrix(train_1), as.matrix(train_2),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 2,
max.depth = 8,
eta = 0.01,
nthread = 8,
nrounds = 1200,
min_child_weight = 1,
subsample = .75,
colsample_bytree = .8,
num_parallel_tree = 1)
#predict gbtree ----------------
pred_xg_train <- predict(xgModel, as.matrix(train_1))
pred_xg_test <- predict(xgModel, as.matrix(test_1))
table(pred_xg_test, as.matrix(test_2))
rm(list = ls())
library(xgboost)
library(randomForest)
library(caret)
library(data.table)
library(lubridate)
library(ggplot2)
library(DiagrammeR)
## THIS IS AN XGBOOST TRAINER USING THE LOGISTIC REGRESSION FILE
dtaMining <- fread("./dta/dtaminingClean_for_logisticRegression.csv", sep = ",")
'sigVars <- read.csv("./dta/SignificantVariables.csv")
sigVars <- sigVars[, 1]
sigVars <- as.vector(sigVars)'
names(dtaMining)[1] <- "label"
'dtaMining <- dtaMining[, c("label", sigVars), with = FALSE]'
d <- 1:nrow(dtaMining)
index <- sample(d, round(nrow(dtaMining)*.8))
#dtaMining[] <- lapply(dtaMining, function(x) as.numeric(x))
train_dta <- dtaMining[index, ]
test_dta <- dtaMining[-index, ]
write.csv(train_dta, "./dta/train.csv", row.names = FALSE)
write.csv(test_dta, "./dta/test.csv", row.names = FALSE)
## XGBOOST ---
train_1 <- train_dta[,2:length(dtaMining)]
test_1 <- test_dta[,2:length(dtaMining)]
train_2 <- train_dta[, 1]
test_2 <- test_dta[, 1]
length(train_2)
train_1[] <- lapply(train_1, function(x) as.numeric(x))
train_2 <- as.numeric(train_2)
test_1[] <- lapply(test_1, function(x) as.numeric(x))
test_2 <- as.numeric(test_2)
xgModel <- xgboost(as.matrix(train_1), as.matrix(train_2),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 2,
max.depth = 8,
eta = 0.01,
nthread = 8,
nrounds = 1200,
min_child_weight = 1,
subsample = .75,
colsample_bytree = .8,
num_parallel_tree = 1)
#predict gbtree ----------------
pred_xg_train <- predict(xgModel, as.matrix(train_1))
pred_xg_test <- predict(xgModel, as.matrix(test_1))
table(pred_xg_test, as.matrix(test_2))
# PART 2 - LOGISTIC REGRESSION
rm(list = ls())
dta <- read.csv("./dta/dtaminingClean_for_logisticRegression.csv")
names(dta)[1] <- "label"
logitModel_1 <- glm(label ~., dta, family = binomial(link = "logit"))
summary(logitModel_1)
rm(list = ls())
library(xgboost)
library(randomForest)
library(caret)
library(data.table)
library(lubridate)
library(ggplot2)
library(DiagrammeR)
## THIS IS AN XGBOOST TRAINER USING THE LOGISTIC REGRESSION FILE
dtaMining <- fread("./dta/dtaminingClean_for_logisticRegression.csv", sep = ",")
sigVars <- read.csv("./dta/SignificantVariables.csv")
sigVars <- sigVars[, 1]
sigVars <- as.vector(sigVars)
names(dtaMining)[1] <- "label"
dtaMining <- dtaMining[, c("label", sigVars), with = FALSE]
d <- 1:nrow(dtaMining)
index <- sample(d, round(nrow(dtaMining)*.8))
#dtaMining[] <- lapply(dtaMining, function(x) as.numeric(x))
train_dta <- dtaMining[index, ]
test_dta <- dtaMining[-index, ]
write.csv(train_dta, "./dta/train.csv", row.names = FALSE)
write.csv(test_dta, "./dta/test.csv", row.names = FALSE)
## XGBOOST ---
train_1 <- train_dta[,2:length(dtaMining)]
test_1 <- test_dta[,2:length(dtaMining)]
train_2 <- train_dta[, 1]
test_2 <- test_dta[, 1]
length(train_2)
train_1[] <- lapply(train_1, function(x) as.numeric(x))
train_2 <- as.numeric(train_2)
test_1[] <- lapply(test_1, function(x) as.numeric(x))
test_2 <- as.numeric(test_2)
xgModel <- xgboost(as.matrix(train_1), as.matrix(train_2),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 2,
max.depth = 8,
eta = 0.01,
nthread = 8,
nrounds = 1200,
min_child_weight = 1,
subsample = .75,
colsample_bytree = .8,
num_parallel_tree = 1)
#predict gbtree ----------------
pred_xg_train <- predict(xgModel, as.matrix(train_1))
pred_xg_test <- predict(xgModel, as.matrix(test_1))
table(pred_xg_test, as.matrix(test_2))
names <- dimnames(train_1)
names
varImportance <- xgb.importance(names, model = xgModel)
xgb.plot.importance(varImportance[1:20,])
names <- dimnames(train_1)[[2]]
varImportance <- xgb.importance(names, model = xgModel)
xgb.plot.importance(varImportance[1:20,])
xgb.plot.importance(varImportance[1:50,])
varImportance
kmeansVarImp <- kmeans(varImportance, 5)
kmeansVarImp <- kmeans(varImportance)
kmeansVarImp <- kmeans(varImportance, centers = 5)
str(varImportance)
?kmeans
kmeansVarImp <- kmeans(varImportance[,-1], centers = 5)
kmeansVarImp
varImportance <- cbind(varImportance, kmeansVarImp$cluster)
varImportance
kmeansVarImp$centers
plot <- ggplot(varImportance, aes(x = Gain, y = Frequency, color = V2)) + geom_point()
plot
plot <- ggplot(varImportance, aes(x = Gain, y = Frequency, fill = V2)) + geom_point()
plot
names(varImportance)
plot <- ggplot(varImportance, aes(x = Gain, y = Frequency, fill = as.factor(V2))) + geom_point()
plot
plot <- ggplot(varImportance, aes(x = Gain, y = Frequency, color = as.factor(V2))) + geom_point()
plot
plot <- ggplot(varImportance, aes(x = Gain, y = Frequency, color = as.factor(V2))) + geom_point(size = 10, alpha = .7)
plot
plot <- ggplot(varImportance, aes(x = Gain, y = Frequency, color = as.factor(V2))) + geom_point(size = 5, alpha = .7)
plot
plot <- ggplot(varImportance, aes(x = Gain, y = Frequency, color = as.factor(V2), size = Gain)) + geom_point(size = 5, alpha = .5)
plot
plot <- ggplot(varImportance, aes(x = Cover, y = Frequency, color = as.factor(V2), size = Gain)) + geom_point(size = 5, alpha = .5)
plot
plot <- ggplot(varImportance[varImportance$V2 != 5, ], aes(x = Cover, y = Frequency, color = as.factor(V2), size = Gain)) + geom_point(size = 5, alpha = .5)
plot
plot <- ggplot(varImportance[varImportance$V2 != 5, ], aes(x = Cover, y = Frequency, size = Gain)) + geom_point(size = 5, alpha = .5)
plot
plot <- ggplot(varImportance[varImportance$V2 != 5, ], aes(x = Cover, y = Frequency, size = Gain, color = as.factor(V2), )) + geom_point(alpha = .5)
plot
varImportance <- varImportance[varImportance$V2 < 3 | varImportance$V2 == 5, ]
varImportance
varImportance <- varImportance$Feature
varImportance
dtaMining <- dtaMining[, varImportance]
dtaMining <- dtaMining[, varImportance, with = FALSE]
d <- 1:nrow(dtaMining)
index <- sample(d, round(nrow(dtaMining)*.8))
#dtaMining[] <- lapply(dtaMining, function(x) as.numeric(x))
train_dta <- dtaMining[index, ]
test_dta <- dtaMining[-index, ]
write.csv(train_dta, "./dta/train.csv", row.names = FALSE)
write.csv(test_dta, "./dta/test.csv", row.names = FALSE)
## XGBOOST ---
train_1 <- train_dta[,2:length(dtaMining)]
test_1 <- test_dta[,2:length(dtaMining)]
train_2 <- train_dta[, 1]
test_2 <- test_dta[, 1]
length(train_2)
train_1[] <- lapply(train_1, function(x) as.numeric(x))
train_2 <- as.numeric(train_2)
test_1[] <- lapply(test_1, function(x) as.numeric(x))
test_2 <- as.numeric(test_2)
xgModel <- xgboost(as.matrix(train_1), as.matrix(train_2),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 2,
max.depth = 8,
eta = 0.01,
nthread = 8,
nrounds = 1200,
min_child_weight = 1,
subsample = .75,
colsample_bytree = .8,
num_parallel_tree = 1)
#predict gbtree ----------------
pred_xg_train <- predict(xgModel, as.matrix(train_1))
pred_xg_test <- predict(xgModel, as.matrix(test_1))
table(pred_xg_test, as.matrix(test_2))
dtaMining <- fread("./dta/dtaminingClean_for_logisticRegression.csv", sep = ",")
dtaMining <- fread("./dta/dtaminingClean_for_logisticRegression.csv", sep = ",")
sigVars <- read.csv("./dta/SignificantVariables.csv")
sigVars <- sigVars[, 1]
sigVars <- as.vector(sigVars)
names(dtaMining)[1] <- "label"
dtaMining <- dtaMining[, c("label", sigVars), with = FALSE]
dtaMining <- dtaMining[, c("label", varImportance), with = FALSE]
d <- 1:nrow(dtaMining)
index <- sample(d, round(nrow(dtaMining)*.8))
#dtaMining[] <- lapply(dtaMining, function(x) as.numeric(x))
train_dta <- dtaMining[index, ]
test_dta <- dtaMining[-index, ]
write.csv(train_dta, "./dta/train.csv", row.names = FALSE)
write.csv(test_dta, "./dta/test.csv", row.names = FALSE)
## XGBOOST ---
train_1 <- train_dta[,2:length(dtaMining)]
test_1 <- test_dta[,2:length(dtaMining)]
train_2 <- train_dta[, 1]
test_2 <- test_dta[, 1]
length(train_2)
train_1[] <- lapply(train_1, function(x) as.numeric(x))
train_2 <- as.numeric(train_2)
test_1[] <- lapply(test_1, function(x) as.numeric(x))
test_2 <- as.numeric(test_2)
xgModel <- xgboost(as.matrix(train_1), as.matrix(train_2),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 2,
max.depth = 8,
eta = 0.01,
nthread = 8,
nrounds = 1200,
min_child_weight = 1,
subsample = .75,
colsample_bytree = .8,
num_parallel_tree = 1)
#predict gbtree ----------------
pred_xg_train <- predict(xgModel, as.matrix(train_1))
pred_xg_test <- predict(xgModel, as.matrix(test_1))
table(pred_xg_test, as.matrix(test_2))
rm(list = ls())
library(xgboost)
library(randomForest)
library(caret)
library(data.table)
library(lubridate)
library(ggplot2)
library(DiagrammeR)
## THIS IS AN XGBOOST TRAINER USING THE LOGISTIC REGRESSION FILE
dtaMining <- fread("./dta/dtaminingClean_for_logisticRegression.csv", sep = ",")
sigVars <- read.csv("./dta/SignificantVariables.csv")
sigVars <- sigVars[, 1]
sigVars <- as.vector(sigVars)
names(dtaMining)[1] <- "label"
dtaMining <- dtaMining[, c("label", sigVars), with = FALSE]
d <- 1:nrow(dtaMining)
index <- sample(d, round(nrow(dtaMining)*.8))
#dtaMining[] <- lapply(dtaMining, function(x) as.numeric(x))
train_dta <- dtaMining[index, ]
test_dta <- dtaMining[-index, ]
write.csv(train_dta, "./dta/train.csv", row.names = FALSE)
write.csv(test_dta, "./dta/test.csv", row.names = FALSE)
## XGBOOST ---
train_1 <- train_dta[,2:length(dtaMining)]
test_1 <- test_dta[,2:length(dtaMining)]
train_2 <- train_dta[, 1]
test_2 <- test_dta[, 1]
length(train_2)
train_1[] <- lapply(train_1, function(x) as.numeric(x))
train_2 <- as.numeric(train_2)
test_1[] <- lapply(test_1, function(x) as.numeric(x))
test_2 <- as.numeric(test_2)
xgModel <- xgboost(as.matrix(train_1), as.matrix(train_2),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 2,
max.depth = 8,
eta = 0.01,
nthread = 8,
nrounds = 1200,
min_child_weight = 1,
subsample = .75,
colsample_bytree = .8,
num_parallel_tree = 1)
#predict gbtree ----------------
pred_xg_train <- predict(xgModel, as.matrix(train_1))
pred_xg_test <- predict(xgModel, as.matrix(test_1))
table(pred_xg_test, as.matrix(test_2))
names <- dimnames(train_1)[[2]]
varImportance <- xgb.importance(names, model = xgModel)
kmeansVarImp <- kmeans(varImportance[,-1], centers = 5)
kmeansVarImp$centers
varImportance <- cbind(varImportance, kmeansVarImp$cluster)
xgb.plot.importance(varImportance[1:50,])
plot <- ggplot(varImportance[varImportance$V2 != 5, ], aes(x = Cover, y = Frequency, size = Gain, color = as.factor(V2))) + geom_point(alpha = .5)
plot
plot <- ggplot(varImportance, aes(x = Cover, y = Frequency, size = Gain, color = as.factor(V2))) + geom_point(alpha = .5)
plot
plot <- ggplot(varImportance[varImportance$V2 != 1, ], aes(x = Cover, y = Frequency, size = Gain, color = as.factor(V2))) + geom_point(alpha = .5)
plot
varImportance <- xgb.importance(names, model = xgModel)
kmeansVarImp <- kmeans(varImportance[,-c(1,3,4)], centers = 5)
kmeansVarImp$centers
varImportance <- cbind(varImportance, kmeansVarImp$cluster)
xgb.plot.importance(varImportance[1:50,])
plot <- ggplot(varImportance[varImportance$V2 != 1, ], aes(x = Cover, y = Frequency, size = Gain, color = as.factor(V2))) + geom_point(alpha = .5)
plot
plot <- ggplot(varImportance, aes(x = Cover, y = Frequency, size = Gain, color = as.factor(V2))) + geom_point(alpha = .5)
plot
plot <- ggplot(varImportance[varImportance$V2 != 4, ], aes(x = Cover, y = Frequency, size = Gain, color = as.factor(V2))) + geom_point(alpha = .5)
plot
varImportance <- varImportance[varImportance$V2 != 5, ]
varImportance <- varImportance$Feature
dtaMining <- dtaMining[, c("label", varImportance), with = FALSE]
d <- 1:nrow(dtaMining)
index <- sample(d, round(nrow(dtaMining)*.8))
#dtaMining[] <- lapply(dtaMining, function(x) as.numeric(x))
train_dta <- dtaMining[index, ]
test_dta <- dtaMining[-index, ]
train_1 <- train_dta[,2:length(dtaMining)]
test_1 <- test_dta[,2:length(dtaMining)]
train_2 <- train_dta[, 1]
test_2 <- test_dta[, 1]
length(train_2)
train_1[] <- lapply(train_1, function(x) as.numeric(x))
train_2 <- as.numeric(train_2)
test_1[] <- lapply(test_1, function(x) as.numeric(x))
test_2 <- as.numeric(test_2)
xgModel <- xgboost(as.matrix(train_1), as.matrix(train_2),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 2,
max.depth = 8,
eta = 0.01,
nthread = 8,
nrounds = 1200,
min_child_weight = 1,
subsample = .75,
colsample_bytree = .8,
num_parallel_tree = 1)
#predict gbtree ----------------
pred_xg_train <- predict(xgModel, as.matrix(train_1))
pred_xg_test <- predict(xgModel, as.matrix(test_1))
table(pred_xg_test, as.matrix(test_2))
names <- dimnames(train_1)[[2]]
varImportance <- xgb.importance(names, model = xgModel)
xgb.plot.importance(varImportance)
xgModel <- xgboost(as.matrix(train_1), as.matrix(train_2),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 2,
max.depth = 8,
eta = 0.001,
nthread = 8,
nrounds = 1200,
min_child_weight = 1,
subsample = .75,
colsample_bytree = .8,
num_parallel_tree = 1)
#predict gbtree ----------------
pred_xg_train <- predict(xgModel, as.matrix(train_1))
pred_xg_test <- predict(xgModel, as.matrix(test_1))
table(pred_xg_test, as.matrix(test_2))
xgModel <- xgboost(as.matrix(train_1), as.matrix(train_2),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 2,
max.depth = 8,
eta = 0.001,
nthread = 8,
nrounds = 1500,
min_child_weight = 1,
subsample = .75,
colsample_bytree = .8,
num_parallel_tree = 1)
#predict gbtree ----------------
pred_xg_train <- predict(xgModel, as.matrix(train_1))
pred_xg_test <- predict(xgModel, as.matrix(test_1))
table(pred_xg_test, as.matrix(test_2))
xgModel <- xgboost(as.matrix(train_1), as.matrix(train_2),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 2,
max.depth = 8,
eta = 0.001,
nthread = 8,
nrounds = 1500,
min_child_weight = 1,
subsample = .75,
colsample_bytree = .8,
num_parallel_tree = 1)
xgModel <- xgboost(as.matrix(train_1), as.matrix(train_2),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 2,
max.depth = 8,
eta = 0.001,
nthread = 8,
nrounds = 1500,
min_child_weight = 1,
subsample = .75,
colsample_bytree = .8,
num_parallel_tree = 1)
install.packages('rattle')
